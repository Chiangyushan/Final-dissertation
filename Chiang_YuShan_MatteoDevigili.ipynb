{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "#### 1. Data Scraping- Cartalk\n",
    "- Due to the large volume of data, the data scraping was divided into 12 separate sessions\n",
    "- The first-try data scraping, there's a mistake for not scraping \"Date\"\n",
    "- Therefore some of the code here are based on the dataset I've scraped, and the re-scraped \"Date\" are added into my previous dataset\n",
    "#### 2. Data Labelling\n",
    "- We don't use it in final project, We do manual labeling at the end\n",
    "#### 3. Name Recognition \n",
    "#### 4. Sentiment Analaysis\n",
    "#### 5. Topic Modelling \n",
    "#### 6. Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data scraping- Cartalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "# Read topic links from the CSV file\n",
    "topic_links = []\n",
    "with open('cartalk_topic_links.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 2000 links\n",
    "topic_links = topic_links[:2000]\n",
    "\n",
    "# List to store the scraped replies\n",
    "replies_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the reply content and date\n",
    "    replies = soup.find_all('div', class_='cooked')  # Assuming reply content is in a div with this class name\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for reply, date in zip(replies, dates):\n",
    "        reply_text = reply.get_text(strip=True)\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        replies_data.append([link, reply_text, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped replies to a new CSV file\n",
    "replies_csv_file = 'cartalk_topic_replies00.csv'\n",
    "with open(replies_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in replies_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All replies have been saved to '{replies_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "replies_file = 'cartalk_topic_replies1.csv'\n",
    "\n",
    "# Read the topic links from the existing replies file\n",
    "topic_links = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 1000 links (adjust as needed)\n",
    "topic_links = topic_links[:1000]\n",
    "\n",
    "# List to store the scraped dates\n",
    "dates_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the dates\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for date in dates:\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        dates_data.append([link, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped dates to a new CSV file\n",
    "dates_csv_file = 'cartalk_topic_dates.csv'\n",
    "with open(dates_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Date'])\n",
    "    \n",
    "    for data in dates_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All dates have been saved to '{dates_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Merge files\n",
    "merged_file = 'cartalk_topic_replies01.csv'\n",
    "\n",
    "# Read the existing replies file\n",
    "replies_data = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    for row in reader:\n",
    "        replies_data.append(row)\n",
    "\n",
    "# Read the dates file and create a dictionary\n",
    "dates_dict = {}\n",
    "with open(dates_csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_link = row[0]\n",
    "        reply_date = row[1]\n",
    "        if topic_link in dates_dict:\n",
    "            dates_dict[topic_link].append(reply_date)\n",
    "        else:\n",
    "            dates_dict[topic_link] = [reply_date]\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for row in replies_data:\n",
    "    topic_link = row[0]\n",
    "    reply = row[1]\n",
    "    date = dates_dict.get(topic_link, [''])[0]  # Default to an empty string if the date doesn't exist\n",
    "    merged_data.append([topic_link, reply, date])\n",
    "    if topic_link in dates_dict:\n",
    "        dates_dict[topic_link].pop(0)  # Remove the used date\n",
    "\n",
    "# Save the merged data\n",
    "with open(merged_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in merged_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All data have been merged and saved to '{merged_file}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "replies_file = 'cartalk_topic_replies2.csv'\n",
    "\n",
    "# Read the topic links from the existing replies file\n",
    "topic_links = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 2000 links (adjust as needed)\n",
    "topic_links = topic_links[:2000]\n",
    "\n",
    "# List to store the scraped dates\n",
    "dates_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the dates\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for date in dates:\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        dates_data.append([link, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped dates to a new CSV file\n",
    "dates_csv_file = 'cartalk_topic_dates2.csv'\n",
    "with open(dates_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Date'])\n",
    "    \n",
    "    for data in dates_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All dates have been saved to '{dates_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Merge files\n",
    "merged_file = 'cartalk_topic_replies02.csv'\n",
    "\n",
    "# Read the existing replies file\n",
    "replies_data = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    for row in reader:\n",
    "        replies_data.append(row)\n",
    "\n",
    "# Read the dates file and create a dictionary\n",
    "dates_dict = {}\n",
    "with open(dates_csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_link = row[0]\n",
    "        reply_date = row[1]\n",
    "        if topic_link in dates_dict:\n",
    "            dates_dict[topic_link].append(reply_date)\n",
    "        else:\n",
    "            dates_dict[topic_link] = [reply_date]\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for row in replies_data:\n",
    "    topic_link = row[0]\n",
    "    reply = row[1]\n",
    "    date = dates_dict.get(topic_link, [''])[0]  # Default to an empty string if the date doesn't exist\n",
    "    merged_data.append([topic_link, reply, date])\n",
    "    if topic_link in dates_dict:\n",
    "        dates_dict[topic_link].pop(0)  # Remove the used date\n",
    "\n",
    "# Save the merged data\n",
    "with open(merged_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in merged_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All data have been merged and saved to '{merged_file}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "replies_file = 'cartalk_topic_replies3.csv'\n",
    "\n",
    "# Read the topic links from the existing replies file\n",
    "topic_links = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 2000 links (adjust as needed)\n",
    "topic_links = topic_links[:2000]\n",
    "\n",
    "# List to store the scraped dates\n",
    "dates_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the dates\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for date in dates:\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        dates_data.append([link, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped dates to a new CSV file\n",
    "dates_csv_file = 'cartalk_topic_dates3.csv'\n",
    "with open(dates_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Date'])\n",
    "    \n",
    "    for data in dates_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All dates have been saved to '{dates_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Merge files\n",
    "merged_file = 'cartalk_topic_replies03.csv'\n",
    "\n",
    "# Read the existing replies file\n",
    "replies_data = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    for row in reader:\n",
    "        replies_data.append(row)\n",
    "\n",
    "# Read the dates file and create a dictionary\n",
    "dates_dict = {}\n",
    "with open(dates_csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_link = row[0]\n",
    "        reply_date = row[1]\n",
    "        if topic_link in dates_dict:\n",
    "            dates_dict[topic_link].append(reply_date)\n",
    "        else:\n",
    "            dates_dict[topic_link] = [reply_date]\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for row in replies_data:\n",
    "    topic_link = row[0]\n",
    "    reply = row[1]\n",
    "    date = dates_dict.get(topic_link, [''])[0]  # Default to an empty string if the date doesn't exist\n",
    "    merged_data.append([topic_link, reply, date])\n",
    "    if topic_link in dates_dict:\n",
    "        dates_dict[topic_link].pop(0)  # Remove the used date\n",
    "\n",
    "# Save the merged data\n",
    "with open(merged_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in merged_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All data have been merged and saved to '{merged_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "replies_file = 'cartalk_topic_replies4.csv'\n",
    "\n",
    "# Read the Topic Links from the existing replies file\n",
    "topic_links = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 2000 links (adjust as needed)\n",
    "topic_links = topic_links[:2000]\n",
    "\n",
    "# List to store the scraped dates\n",
    "dates_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the dates\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for date in dates:\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        dates_data.append([link, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped dates to a new CSV file\n",
    "dates_csv_file = 'cartalk_topic_dates4.csv'\n",
    "with open(dates_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Date'])\n",
    "    \n",
    "    for data in dates_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All dates have been saved to '{dates_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Merge files\n",
    "merged_file = 'cartalk_topic_replies04.csv'\n",
    "\n",
    "# Read the existing replies file\n",
    "replies_data = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    for row in reader:\n",
    "        replies_data.append(row)\n",
    "\n",
    "# Read the dates file and create a dictionary\n",
    "dates_dict = {}\n",
    "with open(dates_csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_link = row[0]\n",
    "        reply_date = row[1]\n",
    "        if topic_link in dates_dict:\n",
    "            dates_dict[topic_link].append(reply_date)\n",
    "        else:\n",
    "            dates_dict[topic_link] = [reply_date]\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for row in replies_data:\n",
    "    topic_link = row[0]\n",
    "    reply = row[1]\n",
    "    date = dates_dict.get(topic_link, [''])[0]  # Default to an empty string if the date doesn't exist\n",
    "    merged_data.append([topic_link, reply, date])\n",
    "    if topic_link in dates_dict:\n",
    "        dates_dict[topic_link].pop(0)  # Remove the used date\n",
    "\n",
    "# Save the merged data\n",
    "with open(merged_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in merged_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All data have been merged and saved to '{merged_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "replies_file = 'cartalk_topic_replies5.csv'\n",
    "\n",
    "# Read the Topic Links from the existing replies file\n",
    "topic_links = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 2000 links (adjust as needed)\n",
    "topic_links = topic_links[:2000]\n",
    "\n",
    "# List to store the scraped dates\n",
    "dates_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the dates\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for date in dates:\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        dates_data.append([link, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped dates to a new CSV file\n",
    "dates_csv_file = 'cartalk_topic_dates5.csv'\n",
    "with open(dates_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Date'])\n",
    "    \n",
    "    for data in dates_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All dates have been saved to '{dates_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Merge files\n",
    "merged_file = 'cartalk_topic_replies05.csv'\n",
    "\n",
    "# Read the existing replies file\n",
    "replies_data = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    for row in reader:\n",
    "        replies_data.append(row)\n",
    "\n",
    "# Read the dates file and create a dictionary\n",
    "dates_dict = {}\n",
    "with open(dates_csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_link = row[0]\n",
    "        reply_date = row[1]\n",
    "        if topic_link in dates_dict:\n",
    "            dates_dict[topic_link].append(reply_date)\n",
    "        else:\n",
    "            dates_dict[topic_link] = [reply_date]\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for row in replies_data:\n",
    "    topic_link = row[0]\n",
    "    reply = row[1]\n",
    "    date = dates_dict.get(topic_link, [''])[0]  # Default to an empty string if the date doesn't exist\n",
    "    merged_data.append([topic_link, reply, date])\n",
    "    if topic_link in dates_dict:\n",
    "        dates_dict[topic_link].pop(0)  # Remove the used date\n",
    "\n",
    "# Save the merged data\n",
    "with open(merged_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in merged_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All data have been merged and saved to '{merged_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "replies_file = 'cartalk_topic_replies6.csv'\n",
    "\n",
    "# Read the Topic Links from the existing replies file\n",
    "topic_links = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 2000 links (adjust as needed)\n",
    "topic_links = topic_links[:2000]\n",
    "\n",
    "# List to store the scraped dates\n",
    "dates_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the dates\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for date in dates:\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        dates_data.append([link, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped dates to a new CSV file\n",
    "dates_csv_file = 'cartalk_topic_dates6.csv'\n",
    "with open(dates_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Date'])\n",
    "    \n",
    "    for data in dates_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All dates have been saved to '{dates_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Merge files\n",
    "merged_file = 'cartalk_topic_replies06.csv'\n",
    "\n",
    "# Read the existing replies file\n",
    "replies_data = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    for row in reader:\n",
    "        replies_data.append(row)\n",
    "\n",
    "# Read the dates file and create a dictionary\n",
    "dates_dict = {}\n",
    "with open(dates_csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_link = row[0]\n",
    "        reply_date = row[1]\n",
    "        if topic_link in dates_dict:\n",
    "            dates_dict[topic_link].append(reply_date)\n",
    "        else:\n",
    "            dates_dict[topic_link] = [reply_date]\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for row in replies_data:\n",
    "    topic_link = row[0]\n",
    "    reply = row[1]\n",
    "    date = dates_dict.get(topic_link, [''])[0]  # Default to an empty string if the date doesn't exist\n",
    "    merged_data.append([topic_link, reply, date])\n",
    "    if topic_link in dates_dict:\n",
    "        dates_dict[topic_link].pop(0)  # Remove the used date\n",
    "\n",
    "# Save the merged data\n",
    "with open(merged_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in merged_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All data have been merged and saved to '{merged_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "replies_file = 'cartalk_topic_replies7.csv'\n",
    "\n",
    "# Read the Topic Links from the existing replies file\n",
    "topic_links = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 2000 links (adjust as needed)\n",
    "topic_links = topic_links[:2000]\n",
    "\n",
    "# List to store the scraped dates\n",
    "dates_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the dates\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for date in dates:\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        dates_data.append([link, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped dates to a new CSV file\n",
    "dates_csv_file = 'cartalk_topic_dates7.csv'\n",
    "with open(dates_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Date'])\n",
    "    \n",
    "    for data in dates_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All dates have been saved to '{dates_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Merge files\n",
    "merged_file = 'cartalk_topic_replies07.csv'\n",
    "\n",
    "# Read the existing replies file\n",
    "replies_data = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    for row in reader:\n",
    "        replies_data.append(row)\n",
    "\n",
    "# Read the dates file and create a dictionary\n",
    "dates_dict = {}\n",
    "with open(dates_csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_link = row[0]\n",
    "        reply_date = row[1]\n",
    "        if topic_link in dates_dict:\n",
    "            dates_dict[topic_link].append(reply_date)\n",
    "        else:\n",
    "            dates_dict[topic_link] = [reply_date]\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for row in replies_data:\n",
    "    topic_link = row[0]\n",
    "    reply = row[1]\n",
    "    date = dates_dict.get(topic_link, [''])[0]  # Default to an empty string if the date doesn't exist\n",
    "    merged_data.append([topic_link, reply, date])\n",
    "    if topic_link in dates_dict:\n",
    "        dates_dict[topic_link].pop(0)  # Remove the used date\n",
    "\n",
    "# Save the merged data\n",
    "with open(merged_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in merged_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All data have been merged and saved to '{merged_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set explicit wait time to 30 seconds\n",
    "replies_file = 'cartalk_topic_replies8.csv'\n",
    "\n",
    "# Read the Topic Links from the existing replies file\n",
    "topic_links = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Only process the first 2000 links (adjust as needed)\n",
    "topic_links = topic_links[:2000]\n",
    "\n",
    "# List to store the scraped dates\n",
    "dates_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the dates\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for date in dates:\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        dates_data.append([link, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped dates to a new CSV file\n",
    "dates_csv_file = 'cartalk_topic_dates8.csv'\n",
    "with open(dates_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Date'])\n",
    "    \n",
    "    for data in dates_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All dates have been saved to '{dates_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Merge files\n",
    "merged_file = 'cartalk_topic_replies08.csv'\n",
    "\n",
    "# Read the existing replies file\n",
    "replies_data = []\n",
    "with open(replies_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    for row in reader:\n",
    "        replies_data.append(row)\n",
    "\n",
    "# Read the dates file and create a dictionary\n",
    "dates_dict = {}\n",
    "with open(dates_csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_link = row[0]\n",
    "        reply_date = row[1]\n",
    "        if topic_link in dates_dict:\n",
    "            dates_dict[topic_link].append(reply_date)\n",
    "        else:\n",
    "            dates_dict[topic_link] = [reply_date]\n",
    "\n",
    "# Merge the data\n",
    "merged_data = []\n",
    "for row in replies_data:\n",
    "    topic_link = row[0]\n",
    "    reply = row[1]\n",
    "    date = dates_dict.get(topic_link, [''])[0]  # Default to an empty string if the date doesn't exist\n",
    "    merged_data.append([topic_link, reply, date])\n",
    "    if topic_link in dates_dict:\n",
    "        dates_dict[topic_link].pop(0)  # Remove the used date\n",
    "\n",
    "# Save the merged data\n",
    "with open(merged_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in merged_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All data have been merged and saved to '{merged_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set an explicit wait time of 30 seconds\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "# Read the topic links from the CSV file\n",
    "topic_links = []\n",
    "with open('cartalk_topic_links.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Process only the links from the 17,000th to the 19,000th\n",
    "topic_links = topic_links[17000:19000]\n",
    "\n",
    "# List to store the scraped replies\n",
    "replies_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find the reply content and dates\n",
    "    replies = soup.find_all('div', class_='cooked')  # Assuming replies are in a div with this class name\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for reply, date in zip(replies, dates):\n",
    "        reply_text = reply.get_text(strip=True)\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        replies_data.append([link, reply_text, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped replies to a new CSV file\n",
    "replies_csv_file = 'cartalk_topic_replies9.csv'\n",
    "with open(replies_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in replies_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All replies have been saved to '{replies_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set an explicit wait time of 30 seconds\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "# Read topic links from the CSV file\n",
    "topic_links = []\n",
    "with open('cartalk_topic_links.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Process only links from index 19000 to 20000\n",
    "topic_links = topic_links[19000:20000]\n",
    "\n",
    "# List to store the scraped replies\n",
    "replies_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find reply content and date\n",
    "    replies = soup.find_all('div', class_='cooked')  # Assuming reply content is in a div with this class name\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for reply, date in zip(replies, dates):\n",
    "        reply_text = reply.get_text(strip=True)\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        replies_data.append([link, reply_text, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped replies to a new CSV file\n",
    "replies_csv_file = 'cartalk_topic_replies10.csv'\n",
    "with open(replies_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in replies_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All replies have been saved to '{replies_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use webdriver_manager to automatically download and manage chromedriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Set an explicit wait time of 30 seconds\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "# Read topic links from the CSV file\n",
    "topic_links = []\n",
    "with open('cartalk_topic_links.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        topic_links.append(row[0])\n",
    "\n",
    "# Process only links from index 20000 to 21000\n",
    "topic_links = topic_links[20000:21000]\n",
    "\n",
    "# List to store the scraped replies\n",
    "replies_data = []\n",
    "\n",
    "for link in topic_links:\n",
    "    # Open each topic link\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Get the page content\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Find reply content and date\n",
    "    replies = soup.find_all('div', class_='cooked')  # Assuming reply content is in a div with this class name\n",
    "    dates = soup.find_all('span', class_='relative-date')  # Assuming dates are in a span with this class name\n",
    "    \n",
    "    for reply, date in zip(replies, dates):\n",
    "        reply_text = reply.get_text(strip=True)\n",
    "        reply_date = date['title'] if 'title' in date.attrs else date.get_text(strip=True)\n",
    "        replies_data.append([link, reply_text, reply_date])\n",
    "    \n",
    "    print(f\"Processed {link}\")\n",
    "\n",
    "# Save the scraped replies to a new CSV file\n",
    "replies_csv_file = 'cartalk_topic_replies11.csv'\n",
    "with open(replies_csv_file, mode='w', newline='', encoding='utf-8') as file: \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Topic Link', 'Reply', 'Date'])\n",
    "    \n",
    "    for data in replies_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"All replies have been saved to '{replies_csv_file}'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "df1 = pd.read_excel('sample_for_manual_labeling.xlsx')\n",
    "\n",
    "# Replace NaN values\n",
    "df1['Reply'] = df1['Reply'].fillna('')\n",
    "\n",
    "# Define the keywords for each category\n",
    "electric_fit_keywords = [\"battery\", \"charged\", \"charge\", \"charging\", \"software\", \"performance\", \"quality\", \"technical\", \"drive\", \"driving\", \"experience\", \"power\", \"torque\", \"autonomous\", \"range\", \"miles\"]\n",
    "affordability_keywords = [\"cost\", \"price\", \"pricing\", \"expensive\", \"cheap\", \"maintenance\", \"repair\", \"expenses\", \"charge cost\", \"£\", \"$\", \"€\", \"money\", \"value\"]\n",
    "customer_care_keywords = [\"dealer\", \"dealership\", \"purchase\", \"buy\", \"after-sales\", \"support\", \"service\", \"customer service\", \"satisfaction\", \"repair\", \"warranty\", \"contact\", \"helpful\", \"unhelpful\"]\n",
    "\n",
    "# Function to label the replies\n",
    "def label_reply_multiple(reply, keywords):\n",
    "    reply = reply.lower() \n",
    "    return any(keyword in reply for keyword in keywords)\n",
    "\n",
    "# Apply the labeling function for each category\n",
    "df1['Electric Fit'] = df1['Reply'].apply(lambda x: 1 if label_reply_multiple(x, electric_fit_keywords) else 0)\n",
    "df1['Affordability'] = df1['Reply'].apply(lambda x: 1 if label_reply_multiple(x, affordability_keywords) else 0)\n",
    "df1['Customer Care'] = df1['Reply'].apply(lambda x: 1 if label_reply_multiple(x, customer_care_keywords) else 0)\n",
    "\n",
    "# Save the labeled data to a new Excel file\n",
    "output_file_path = 'multi_labelled.xlsx'\n",
    "df1.to_excel(output_file_path, index=False)\n",
    "\n",
    "output_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Name Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version I've tried before discuss with group members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Register and add the TextBlob component\n",
    "spacy_text_blob = SpacyTextBlob(nlp)\n",
    "nlp.add_pipe(\"spacytextblob\", last=True)\n",
    "\n",
    "# Load the dataset\n",
    "df_new = pd.read_csv('new_cleaned_filtered_main_removed_missing_duplicates_with_predictions.csv')\n",
    "\n",
    "# Perform sentiment analysis using SpaCy and TextBlob\n",
    "if 'Review' in df_new.columns:\n",
    "    def get_sentiment(text):\n",
    "        doc = nlp(text)\n",
    "        return doc._.polarity, doc._.subjectivity\n",
    "\n",
    "    df_new['Polarity'], df_new['Subjectivity'] = zip(*df_new['Review'].apply(get_sentiment))\n",
    "\n",
    "# Keywords to filter and count\n",
    "keywords = ['mini', 'nissan', 'vauxhall', 'honda', 'peugeot', 'renault', 'zoe', 'leaf', 'bmw', 'i3', 'hyundai', 'kona', 'chevrolet', 'bolt', 'fiat', '500e', 'cooper se']\n",
    "\n",
    "# Function to recognize car brands and ensure case-insensitive matching\n",
    "def recognize_brand(text, brands):\n",
    "    text_lower = text.lower()\n",
    "    for brand in brands:\n",
    "        if brand.lower() in text_lower:\n",
    "            return brand\n",
    "    return None\n",
    "\n",
    "# Apply the function to each review to find the car brand\n",
    "df_new['Car Brand'] = df_new['Review'].apply(lambda x: recognize_brand(x, keywords))\n",
    "\n",
    "# Function to filter reviews mentioning specified keywords and count occurrences\n",
    "def count_reviews_with_keywords(df, keywords):\n",
    "    keywords_lower = [keyword.lower() for keyword in keywords]  # Ensure all keywords are lowercase\n",
    "    def contains_keyword(review):\n",
    "        review_lower = str(review).lower()\n",
    "        return any(keyword in review_lower for keyword in keywords_lower)\n",
    "    \n",
    "    filtered_df = df[df['Review'].apply(contains_keyword)]\n",
    "    counts = filtered_df['Review'].str.lower().apply(lambda x: [keyword for keyword in keywords_lower if keyword in x]).explode().value_counts()\n",
    "    \n",
    "    return filtered_df, counts\n",
    "\n",
    "# Get the filtered reviews and counts with the combined approach\n",
    "filtered_reviews, keyword_counts = count_reviews_with_keywords(df_new, keywords)\n",
    "\n",
    "# Convert keyword counts to DataFrame for better visibility\n",
    "keyword_counts_df = keyword_counts.reset_index()\n",
    "keyword_counts_df.columns = ['Keyword', 'Count']\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_new.to_csv('updated_reviews_with_sentiment_and_car_brand_combined.csv', index=False)\n",
    "\n",
    "# Print the top 5 most common car brands and all recognized brands\n",
    "print(\"Top 5 most common car brands:\\n\", keyword_counts_df.head())\n",
    "print(\"\\nFrequency of all recognized brands:\\n\", keyword_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('new_cleaned_filtered_main_removed_missing_duplicates_with_predictions.csv')\n",
    "# Dictionary mapping models to their respective brands\n",
    "model_to_brand = {\n",
    "    'mini': 'Mini', 'cooper se': 'Mini',\n",
    "    'nissan': 'Nissan', 'leaf': 'Nissan',\n",
    "    'vauxhall': 'Vauxhall', 'corsa': 'Vauxhall',\n",
    "    'honda': 'Honda',\n",
    "    'peugeot': 'Peugeot', '208': 'Peugeot', 'e-208': 'Peugeot',\n",
    "    'renault': 'Renault', 'zoe': 'Renault',\n",
    "    'fiat': 'Fiat', '500e': 'Fiat',\n",
    "    'bmw': 'BMW', 'i3': 'BMW',\n",
    "    'hyundai': 'Hyundai', 'kona': 'Hyundai',\n",
    "    'chevrolet': 'Chevrolet', 'bolt': 'Chevrolet'\n",
    "}\n",
    "\n",
    "# Improved function to include variations in keyword case and relate brands and models\n",
    "def count_reviews_with_keywords(df, model_to_brand):\n",
    "    keywords_lower = {k.lower(): v for k, v in model_to_brand.items()}  # Lowercase keywords and map to brand\n",
    "    def contains_keyword(review):\n",
    "        review_lower = str(review).lower()\n",
    "        return any(keyword in review_lower for keyword in keywords_lower)\n",
    "    \n",
    "    filtered_df = df[df['Review'].apply(contains_keyword)]\n",
    "    counts = filtered_df['Review'].str.lower().apply(lambda x: [keywords_lower[keyword] for keyword in keywords_lower if keyword in x]).explode().value_counts()\n",
    "    \n",
    "    return filtered_df, counts\n",
    "\n",
    "# Get the filtered reviews and counts with improved function\n",
    "filtered_reviews, keyword_counts = count_reviews_with_keywords(df_new, model_to_brand)\n",
    "\n",
    "# Convert keyword counts to DataFrame for better visibility\n",
    "keyword_counts_df = keyword_counts.reset_index()\n",
    "keyword_counts_df.columns = ['Brand', 'Count']\n",
    "\n",
    "# Display the results\n",
    "filtered_reviews.head(), keyword_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final version of Name recognition part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_new=pd.read_csv('new_cleaned_filtered_main_removed_missing_duplicates_with_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Brand Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping models to their respective brands\n",
    "model_to_brand = {\n",
    "    'mini': 'Mini', 'cooper se': 'Mini',\n",
    "    'nissan': 'Nissan', 'leaf': 'Nissan',\n",
    "    'vauxhall': 'Vauxhall', 'corsa': 'Vauxhall',\n",
    "    'honda': 'Honda','honda e': 'Honda',\n",
    "    'peugeot': 'Peugeot', '208': 'Peugeot', 'e-208': 'Peugeot',\n",
    "    'renault': 'Renault', 'zoe': 'Renault',\n",
    "    'fiat': 'Fiat', '500e': 'Fiat',\n",
    "    'bmw': 'BMW', 'i3': 'BMW',\n",
    "    'hyundai': 'Hyundai', 'kona': 'Hyundai',\n",
    "    'chevrolet': 'Chevrolet', 'bolt': 'Chevrolet'\n",
    "}\n",
    "\n",
    "# Improved function to include variations in keyword case and relate brands and models\n",
    "def count_reviews_with_keywords(df, model_to_brand):\n",
    "    keywords_lower = {k.lower(): v for k, v in model_to_brand.items()}  # Lowercase keywords and map to brand\n",
    "    def contains_keyword(review):\n",
    "        review_lower = str(review).lower()\n",
    "        return any(keyword in review_lower for keyword in keywords_lower)\n",
    "    \n",
    "    filtered_df = df[df['Review'].apply(contains_keyword)]\n",
    "    counts = filtered_df['Review'].str.lower().apply(lambda x: [keywords_lower[keyword] for keyword in keywords_lower if keyword in x]).explode().value_counts()\n",
    "    \n",
    "    return filtered_df, counts\n",
    "\n",
    "# Get the filtered reviews and counts with improved function\n",
    "filtered_reviews, keyword_counts = count_reviews_with_keywords(df_new, model_to_brand)\n",
    "\n",
    "# Convert keyword counts to DataFrame for better visibility\n",
    "keyword_counts_df = keyword_counts.reset_index()\n",
    "keyword_counts_df.columns = ['Brand', 'Count']\n",
    "\n",
    "# Display the results\n",
    "filtered_reviews.head(), keyword_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Filter Fiat and Competitors(Mini, Honda, Vauxhall, Peugeot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping models to their respective brands\n",
    "model_to_brand = {\n",
    "    'mini': 'Mini', 'cooper se': 'Mini',\n",
    "    'nissan': 'Nissan', 'leaf': 'Nissan',\n",
    "    'vauxhall': 'Vauxhall', 'corsa': 'Vauxhall',\n",
    "    'honda': 'Honda', 'honda e': 'Honda',\n",
    "    'peugeot': 'Peugeot', '208': 'Peugeot', 'e-208': 'Peugeot',\n",
    "    'renault': 'Renault', 'zoe': 'Renault',\n",
    "    'fiat': 'Fiat', '500e': 'Fiat',\n",
    "    'bmw': 'BMW', 'i3': 'BMW',\n",
    "    'hyundai': 'Hyundai', 'kona': 'Hyundai',\n",
    "    'chevrolet': 'Chevrolet', 'bolt': 'Chevrolet'\n",
    "}\n",
    "\n",
    "# Function to label reviews with brands\n",
    "def label_reviews_with_brands(df, model_to_brand):\n",
    "    keywords_lower = {k.lower(): v for k, v in model_to_brand.items()}  # Lowercase keywords and map to brand\n",
    "    \n",
    "    def find_brand(review):\n",
    "        review_lower = str(review).lower()\n",
    "        for keyword, brand in keywords_lower.items():\n",
    "            if keyword in review_lower:\n",
    "                return brand\n",
    "        return None\n",
    "    \n",
    "    df['Brand'] = df['Review'].apply(find_brand)\n",
    "    \n",
    "    # Filter for only the desired brands\n",
    "    desired_brands = ['Mini', 'Honda', 'Vauxhall', 'Peugeot','Fiat']\n",
    "    df_filtered = df[df['Brand'].isin(desired_brands)]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_filtered = label_reviews_with_brands(df_new, model_to_brand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Final Dataset\n",
    "Remove the unclear date, and assume it as our final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new file for analysis\n",
    "new_cleaned_filtered_df = pd.read_csv('finaldf_V2.csv')\n",
    "\n",
    "# Display unique values in the 'Date' column to identify any non-date entries\n",
    "unique_dates = new_cleaned_filtered_df['Date'].unique()\n",
    "unique_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe and convert the 'Date' column to strings\n",
    "new_cleaned_filtered_df = pd.read_csv('finaldf_V2.csv', dtype={'Author': str})\n",
    "new_cleaned_filtered_df['Date'] = new_cleaned_filtered_df['Date'].astype(str)\n",
    "\n",
    "# Identify rows with non-date entries in the 'Date' column\n",
    "non_date_rows = new_cleaned_filtered_df[~new_cleaned_filtered_df['Date'].str.match(r'^\\d{4}/\\d{2}/\\d{2}$')]\n",
    "\n",
    "# Remove these rows from the main dataframe\n",
    "cleaned_df = new_cleaned_filtered_df[new_cleaned_filtered_df['Date'].str.match(r'^\\d{4}/\\d{2}/\\d{2}$')]\n",
    "\n",
    "# Save the non-date rows to a new file\n",
    "non_date_rows.to_csv('non_date_entries1.csv', index=False)\n",
    "\n",
    "# Save the cleaned dataframe to a new file\n",
    "cleaned_df.to_csv('final.csv', index=False)\n",
    "\n",
    "# Display the non-date rows\n",
    "print(non_date_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data header\n",
    "# Load the cleaned dataset\n",
    "cleaned_df = pd.read_csv('final.csv')\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "cleaned_df['Date'] = pd.to_datetime(cleaned_df['Date'])\n",
    "\n",
    "# Display the first few rows to verify\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('final.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "# Apply sentiment analysis on each review\n",
    "df['Sentiment_Score'] = df['Cleaned_Review'].apply(analyze_sentiment)\n",
    "\n",
    "# Define the labels of interest as a dictionary with their keywords\n",
    "labels_of_interest = {\n",
    "    'Electric Fit': 'Electric Fit',\n",
    "    'Affordability': 'Affordability',\n",
    "    'Customer Care': 'Customer Care'\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "df_filtered = pd.DataFrame()\n",
    "\n",
    "# Iterate over each category and filter the DataFrame\n",
    "for category, keyword in labels_of_interest.items():\n",
    "    # Filter rows where the Predicted_Label contains the keyword\n",
    "    df_category = df[df['Predicted_Label'].str.contains(keyword, case=False, na=False)].copy()\n",
    "    # Assign the category to a new column, adding to any existing categories\n",
    "    if 'Category' in df_category.columns:\n",
    "        df_category['Category'] = df_category['Category'] + ', ' + category\n",
    "    else:\n",
    "        df_category['Category'] = category\n",
    "    # Append the results to the final DataFrame\n",
    "    df_filtered = pd.concat([df_filtered, df_category])\n",
    "\n",
    "# Group by brand and category, then calculate mean sentiment score\n",
    "sentiment_by_category = df_filtered.groupby(['Brand', 'Category'])['Sentiment_Score'].mean().unstack()\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(sentiment_by_category, annot=True, cmap='coolwarm')\n",
    "plt.title('Sentiment Analysis by Brand and Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Register and add the TextBlob component\n",
    "spacy_text_blob = SpacyTextBlob(nlp)\n",
    "nlp.add_pipe(\"spacytextblob\", last=True)\n",
    "\n",
    "# Function to process reviews based on a specific label and perform sentiment analysis\n",
    "def process_reviews_by_label(df, label, output_filename):\n",
    "    # Filter reviews where the label is present in the 'Predicted_Label'\n",
    "    filtered_reviews = df[df['Predicted_Label'].apply(lambda x: label in x)]\n",
    "    \n",
    "    # Perform Sentiment Analysis using Cleaned_Review column with SpaCy and spacytextblob\n",
    "    def get_sentiment(text):\n",
    "        doc = nlp(text)\n",
    "        if doc._.polarity > 0:\n",
    "            return 'positive'\n",
    "        elif doc._.polarity < 0:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "    filtered_reviews['Sentiment'] = filtered_reviews['Cleaned_Review'].apply(get_sentiment)\n",
    "\n",
    "    # Aggregate results by car brand and sentiment\n",
    "    sentiment_summary = filtered_reviews.groupby(['Brand', 'Sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Add a 'Total' column to summarize the number of reviews for each sentiment category\n",
    "    sentiment_summary['Total'] = sentiment_summary.sum(axis=1)\n",
    "    \n",
    "    # Save the filtered reviews with sentiment analysis to a new CSV file\n",
    "    filtered_reviews.to_csv(output_filename, index=False)\n",
    "\n",
    "    # Display the path to the saved dataset\n",
    "    print(f\"Dataset with sentiments saved to: {output_filename}\")\n",
    "    return sentiment_summary\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('final.csv')\n",
    "\n",
    "# Process reviews for different labels\n",
    "labels = ['Electric Fit', 'Affordability', 'Customer Care']\n",
    "output_files = ['electric_fit.csv', 'affordability.csv', 'customer_care.csv']\n",
    "\n",
    "for label, output_file in zip(labels, output_files):\n",
    "    sentiment_summary = process_reviews_by_label(df, label, output_file)\n",
    "    print(f\"\\nSentiment summary for {label}:\")\n",
    "    print(sentiment_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Visualise in Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Load the datasets\n",
    "electric_fit_df = pd.read_csv('electric_fit.csv')\n",
    "customer_care_df = pd.read_csv('customer_care.csv')\n",
    "affordability_df = pd.read_csv('affordability.csv')\n",
    "\n",
    "# Function to visualize sentiment distribution\n",
    "def visualize_sentiment_distribution(df, kpi_name):\n",
    "    sentiment_summary = df.groupby(['Brand', 'Sentiment']).size().unstack(fill_value=0)\n",
    "    sentiment_summary.plot(kind='bar', stacked=True, figsize=(10, 6), title=f'Sentiment Distribution for {kpi_name}')\n",
    "    plt.xlabel('Brand')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize each KPI\n",
    "visualize_sentiment_distribution(electric_fit_df, 'Electric Fit')\n",
    "visualize_sentiment_distribution(customer_care_df, 'Customer Care')\n",
    "visualize_sentiment_distribution(affordability_df, 'Affordability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Visualise in Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Load the datasets\n",
    "electric_fit_df = pd.read_csv('electric_fit.csv')\n",
    "customer_care_df = pd.read_csv('customer_care.csv')\n",
    "affordability_df = pd.read_csv('affordability.csv')\n",
    "\n",
    "# Function to visualize sentiment distribution as percentages\n",
    "def visualize_sentiment_distribution(df, kpi_name):\n",
    "    sentiment_summary = df.groupby(['Brand', 'Sentiment']).size().unstack(fill_value=0)\n",
    "    sentiment_percentage = sentiment_summary.div(sentiment_summary.sum(axis=1), axis=0) * 100\n",
    "    sentiment_percentage.plot(kind='bar', stacked=True, figsize=(10, 6), title=f'Sentiment Distribution for {kpi_name} (Percentage)')\n",
    "    plt.xlabel('Brand')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize each KPI as percentage\n",
    "visualize_sentiment_distribution(electric_fit_df, 'Electric Fit')\n",
    "visualize_sentiment_distribution(customer_care_df, 'Customer Care')\n",
    "visualize_sentiment_distribution(affordability_df, 'Affordability')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Included others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary  # Import the Dictionary class\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from pyLDAvis import display\n",
    "\n",
    "# Function to split KPIs from the Predicted_Label column\n",
    "def split_kpis(predicted_label):\n",
    "    return predicted_label.strip(\"()\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "# Apply the split function to create a KPI column\n",
    "df['KPI'] = df['Predicted_Label'].apply(split_kpis)\n",
    "\n",
    "# Explode the KPI column to have one KPI per row\n",
    "df_exploded = df.explode('KPI')\n",
    "\n",
    "# Now you can split the data into Fiat and competitors\n",
    "fiat_df = df_exploded[df_exploded['Brand'] == 'Fiat']\n",
    "competitors_df = df_exploded[df_exploded['Brand'] != 'Fiat']\n",
    "\n",
    "print(f\"Fiat dataset contains {len(fiat_df)} reviews.\")\n",
    "print(f\"Competitors dataset contains {len(competitors_df)} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Fiat\n",
    "fiat_reviews = fiat_df['Cleaned_Review'].tolist()\n",
    "fiat_texts = [review.split() for review in fiat_reviews]  # Tokenize the cleaned reviews\n",
    "fiat_id2word = Dictionary(fiat_texts)\n",
    "fiat_corpus = [fiat_id2word.doc2bow(text) for text in fiat_texts]\n",
    "\n",
    "# For Competitors\n",
    "competitors_reviews = competitors_df['Cleaned_Review'].tolist()\n",
    "competitors_texts = [review.split() for review in competitors_reviews]  # Tokenize the cleaned reviews\n",
    "competitors_id2word = Dictionary(competitors_texts)\n",
    "competitors_corpus = [competitors_id2word.doc2bow(text) for text in competitors_texts]\n",
    "\n",
    "print('Fiat corpus contains {} reviews with {} unique words'.format(len(fiat_corpus), len(fiat_id2word)))\n",
    "print('Competitors corpus contains {} reviews with {} unique words'.format(len(competitors_corpus), len(competitors_id2word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics\n",
    "num_topics = 15\n",
    "\n",
    "# Train LDA model for Fiat\n",
    "fiat_lda_model = LdaModel(fiat_corpus, num_topics=num_topics, id2word=fiat_id2word, passes=15)\n",
    "print(\"Fiat LDA Model:\")\n",
    "for idx, topic in fiat_lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Train LDA model for Competitors\n",
    "competitors_lda_model = LdaModel(competitors_corpus, num_topics=num_topics, id2word=competitors_id2word, passes=15)\n",
    "print(\"\\nCompetitors LDA Model:\")\n",
    "for idx, topic in competitors_lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Fiat LDA Model\n",
    "fiat_lda_vis = gensimvis.prepare(fiat_lda_model, fiat_corpus, fiat_id2word)\n",
    "display(fiat_lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Competitors LDA Model\n",
    "competitors_lda_vis = gensimvis.prepare(competitors_lda_model, competitors_corpus, competitors_id2word)\n",
    "display(competitors_lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract topics\n",
    "def extract_topics(reviews, model, id2word, num_topics):\n",
    "    topics = []\n",
    "    for review in reviews:\n",
    "        bow = id2word.doc2bow(review.split())\n",
    "        doc_topics = model.get_document_topics(bow)\n",
    "        topic_vector = [0] * num_topics\n",
    "        for topic_num, score in doc_topics:\n",
    "            topic_vector[topic_num] = score\n",
    "        topics.append(topic_vector)\n",
    "    return topics\n",
    "\n",
    "# Extract topics for Fiat\n",
    "fiat_df['Topics'] = extract_topics(fiat_reviews, fiat_lda_model, fiat_id2word, num_topics)\n",
    "\n",
    "# Extract topics for Competitors\n",
    "competitors_df['Topics'] = extract_topics(competitors_reviews, competitors_lda_model, competitors_id2word, num_topics)\n",
    "\n",
    "# Convert 'Topics' column to a DataFrame with each topic as a separate column\n",
    "fiat_topics_df = pd.DataFrame(fiat_df['Topics'].tolist(), columns=['Fiat_Topic_' + str(i) for i in range(num_topics)])\n",
    "competitors_topics_df = pd.DataFrame(competitors_df['Topics'].tolist(), columns=['Competitors_Topic_' + str(i) for i in range(num_topics)])\n",
    "\n",
    "# Concatenate the topics DataFrame with the original DataFrame\n",
    "fiat_df = pd.concat([fiat_df.reset_index(drop=True), fiat_topics_df], axis=1)\n",
    "competitors_df = pd.concat([competitors_df.reset_index(drop=True), competitors_topics_df], axis=1)\n",
    "\n",
    "# Calculate average topic distributions for Fiat and competitors\n",
    "# We only want to calculate the mean for the topic columns, which are numeric\n",
    "fiat_topic_columns = [col for col in fiat_df.columns if col.startswith('Fiat_Topic_')]\n",
    "competitors_topic_columns = [col for col in competitors_df.columns if col.startswith('Competitors_Topic_')]\n",
    "\n",
    "fiat_averages = fiat_df[fiat_topic_columns].mean()\n",
    "competitors_averages = competitors_df[competitors_topic_columns].mean()\n",
    "\n",
    "print(f\"Fiat Average Topic Distributions:\\n{fiat_averages}\")\n",
    "print(f\"Competitors Average Topic Distributions:\\n{competitors_averages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  5.2 Exclude Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from pyLDAvis import display\n",
    "\n",
    "# Function to split KPIs from the Predicted_Label column\n",
    "def split_kpis(predicted_label):\n",
    "    return predicted_label.strip(\"()\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "# Apply the split function to create a KPI column\n",
    "df['KPI'] = df['Predicted_Label'].apply(split_kpis)\n",
    "\n",
    "# Explode the KPI column to have one KPI per row\n",
    "df_exploded = df.explode('KPI')\n",
    "\n",
    "# Exclude the \"Other\" label from analysis\n",
    "df_exploded = df_exploded[df_exploded['KPI'] != 'Other']\n",
    "\n",
    "# Split the data into Fiat and competitors\n",
    "fiat_df = df_exploded[df_exploded['Brand'] == 'Fiat']\n",
    "competitors_df = df_exploded[df_exploded['Brand'] != 'Fiat']\n",
    "\n",
    "print(f\"Fiat dataset contains {len(fiat_df)} reviews.\")\n",
    "print(f\"Competitors dataset contains {len(competitors_df)} reviews.\")\n",
    "\n",
    "# Build the dataset (corpus) for Fiat\n",
    "fiat_reviews = fiat_df['Cleaned_Review'].tolist()\n",
    "fiat_texts = [review.split() for review in fiat_reviews]  # Tokenize the cleaned reviews\n",
    "fiat_id2word = Dictionary(fiat_texts)\n",
    "fiat_corpus = [fiat_id2word.doc2bow(text) for text in fiat_texts]\n",
    "print('Fiat corpus contains {} reviews with {} unique words'.format(len(fiat_corpus), len(fiat_id2word)))\n",
    "\n",
    "# Train the topic model for Fiat\n",
    "num_topics = 15  # Number of topics\n",
    "fiat_lda_model = LdaModel(fiat_corpus, num_topics=num_topics, id2word=fiat_id2word, passes=15)\n",
    "\n",
    "# Print all topics for Fiat\n",
    "print(\"Fiat LDA Model:\")\n",
    "for idx, topic in fiat_lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Build the dataset (corpus) for Competitors\n",
    "competitors_reviews = competitors_df['Cleaned_Review'].tolist()\n",
    "competitors_texts = [review.split() for review in competitors_reviews]  # Tokenize the cleaned reviews\n",
    "competitors_id2word = Dictionary(competitors_texts)\n",
    "competitors_corpus = [competitors_id2word.doc2bow(text) for text in competitors_texts]\n",
    "print('Competitors corpus contains {} reviews with {} unique words'.format(len(competitors_corpus), len(competitors_id2word)))\n",
    "\n",
    "# Train the topic model for Competitors\n",
    "competitors_lda_model = LdaModel(competitors_corpus, num_topics=num_topics, id2word=competitors_id2word, passes=15)\n",
    "\n",
    "# Print all topics for Competitors\n",
    "print(\"\\nCompetitors LDA Model:\")\n",
    "for idx, topic in competitors_lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics using pyLDAvis for Fiat\n",
    "fiat_lda_vis = gensimvis.prepare(fiat_lda_model, fiat_corpus, fiat_id2word)\n",
    "display(fiat_lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics using pyLDAvis for Competitors\n",
    "competitors_lda_vis = gensimvis.prepare(competitors_lda_model, competitors_corpus, competitors_id2word)\n",
    "display(competitors_lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract topics from reviews\n",
    "def extract_topics(reviews, model, id2word, num_topics):\n",
    "    topics = []\n",
    "    for review in reviews:\n",
    "        bow = id2word.doc2bow(review.split())\n",
    "        doc_topics = model.get_document_topics(bow)\n",
    "        topic_vector = [0] * num_topics\n",
    "        for topic_num, score in doc_topics:\n",
    "            topic_vector[topic_num] = score\n",
    "        topics.append(topic_vector)\n",
    "    return topics\n",
    "\n",
    "# Extract topics for Fiat\n",
    "fiat_df['Topics'] = extract_topics(fiat_reviews, fiat_lda_model, fiat_id2word, num_topics)\n",
    "\n",
    "# Extract topics for Competitors\n",
    "competitors_df['Topics'] = extract_topics(competitors_reviews, competitors_lda_model, competitors_id2word, num_topics)\n",
    "\n",
    "# Convert 'Topics' column to a DataFrame with each topic as a separate column for Fiat\n",
    "fiat_topics_df = pd.DataFrame(fiat_df['Topics'].tolist(), columns=['Fiat_Topic_' + str(i) for i in range(num_topics)])\n",
    "\n",
    "# Convert 'Topics' column to a DataFrame with each topic as a separate column for Competitors\n",
    "competitors_topics_df = pd.DataFrame(competitors_df['Topics'].tolist(), columns=['Competitors_Topic_' + str(i) for i in range(num_topics)])\n",
    "\n",
    "# Concatenate the topics DataFrame with the original DataFrame for Fiat\n",
    "fiat_df = pd.concat([fiat_df.reset_index(drop=True), fiat_topics_df], axis=1)\n",
    "\n",
    "# Concatenate the topics DataFrame with the original DataFrame for Competitors\n",
    "competitors_df = pd.concat([competitors_df.reset_index(drop=True), competitors_topics_df], axis=1)\n",
    "\n",
    "# Ensure that the topic columns are numeric\n",
    "fiat_topic_columns = ['Fiat_Topic_' + str(i) for i in range(num_topics)]\n",
    "competitors_topic_columns = ['Competitors_Topic_' + str(i) for i in range(num_topics)]\n",
    "\n",
    "fiat_df[fiat_topic_columns] = fiat_df[fiat_topic_columns].apply(pd.to_numeric)\n",
    "competitors_df[competitors_topic_columns] = competitors_df[competitors_topic_columns].apply(pd.to_numeric)\n",
    "\n",
    "# Calculate average topic distributions for Fiat and Competitors\n",
    "fiat_averages = fiat_df[fiat_topic_columns].mean()\n",
    "competitors_averages = competitors_df[competitors_topic_columns].mean()\n",
    "\n",
    "print(f\"Fiat Average Topic Distributions:\\n{fiat_averages}\")\n",
    "print(f\"Competitors Average Topic Distributions:\\n{competitors_averages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've try the basic time-series at first, Due to the distribution of tasks, our group member take over it so that i didnt extend the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "cleaned_df = pd.read_csv('final.csv')\n",
    "cleaned_df['Date'] = pd.to_datetime(cleaned_df['Date'], errors='coerce')\n",
    "\n",
    "# Load the sentiment data files\n",
    "ef_df = pd.read_csv('electric_fit_filtered_reviews.csv')\n",
    "af_df = pd.read_csv('affordability_filtered_reviews.csv')\n",
    "ccf_df = pd.read_csv('customer_care_filtered_reviews.csv')\n",
    "\n",
    "# Convert Date columns to datetime\n",
    "ef_df['Date'] = pd.to_datetime(ef_df['Date'], errors='coerce')\n",
    "af_df['Date'] = pd.to_datetime(af_df['Date'], errors='coerce')\n",
    "ccf_df['Date'] = pd.to_datetime(ccf_df['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot sentiment trends\n",
    "def plot_sentiment_trends(df, title):\n",
    "    sentiment_trends = df.groupby(['Date', 'Sentiment']).size().unstack(fill_value=0)\n",
    "    sentiment_trends.plot(figsize=(14, 7))\n",
    "    plt.title(f'{title} Sentiment Trends Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot sentiment trends for Electric Fit\n",
    "plot_sentiment_trends(ef_df, 'Electric Fit')\n",
    "\n",
    "# Plot sentiment trends for Affordability\n",
    "plot_sentiment_trends(af_df, 'Affordability')\n",
    "\n",
    "# Plot sentiment trends for Customer Care\n",
    "plot_sentiment_trends(ccf_df, 'Customer Care')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot smoothed sentiment trends\n",
    "def plot_smoothed_sentiment_trends(df, title, window=30):\n",
    "    sentiment_trends = df.groupby(['Date', 'Sentiment']).size().unstack(fill_value=0)\n",
    "    sentiment_trends = sentiment_trends.rolling(window=window).mean()\n",
    "    \n",
    "    sentiment_trends.plot(figsize=(14, 7))\n",
    "    plt.title(f'{title} Sentiment Trends Over Time (Smoothed)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot smoothed sentiment trends for Electric Fit\n",
    "plot_smoothed_sentiment_trends(ef_df, 'Electric Fit')\n",
    "\n",
    "# Plot smoothed sentiment trends for Affordability\n",
    "plot_smoothed_sentiment_trends(af_df, 'Affordability')\n",
    "\n",
    "# Plot smoothed sentiment trends for Customer Care\n",
    "plot_smoothed_sentiment_trends(ccf_df, 'Customer Care')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot sentiment trends separately\n",
    "def plot_separate_sentiment_trends(df, title):\n",
    "    sentiment_trends = df.groupby(['Date', 'Sentiment']).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 14), sharex=True)\n",
    "    sentiments = ['positive', 'neutral', 'negative']\n",
    "    colors = ['green', 'orange', 'red']\n",
    "    \n",
    "    for sentiment, color, ax in zip(sentiments, colors, axes):\n",
    "        sentiment_trend = sentiment_trends[sentiment].rolling(window=30).mean()\n",
    "        ax.plot(sentiment_trend.index, sentiment_trend, label=sentiment, color=color)\n",
    "        ax.set_title(f'{title} - {sentiment.capitalize()} Sentiment')\n",
    "        ax.set_ylabel('Number of Reviews')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.show()\n",
    "\n",
    "# Plot separate sentiment trends for Electric Fit\n",
    "plot_separate_sentiment_trends(ef_df, 'Electric Fit')\n",
    "\n",
    "# Plot separate sentiment trends for Affordability\n",
    "plot_separate_sentiment_trends(af_df, 'Affordability')\n",
    "\n",
    "# Plot separate sentiment trends for Customer Care\n",
    "plot_separate_sentiment_trends(ccf_df, 'Customer Care')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot stacked area sentiment trends\n",
    "def plot_stacked_area_sentiment_trends(df, title):\n",
    "    sentiment_trends = df.groupby(['Date', 'Sentiment']).size().unstack(fill_value=0)\n",
    "    sentiment_trends = sentiment_trends.rolling(window=30).mean()\n",
    "    \n",
    "    sentiment_trends.plot.area(figsize=(14, 7), stacked=True)\n",
    "    plt.title(f'{title} Sentiment Trends Over Time (Stacked Area)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot stacked area sentiment trends for Electric Fit\n",
    "plot_stacked_area_sentiment_trends(ef_df, 'Electric Fit')\n",
    "\n",
    "# Plot stacked area sentiment trends for Affordability\n",
    "plot_stacked_area_sentiment_trends(af_df, 'Affordability')\n",
    "\n",
    "# Plot stacked area sentiment trends for Customer Care\n",
    "plot_stacked_area_sentiment_trends(ccf_df, 'Customer Care')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
